{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Resume a Keras MNIST ConvNet Model\n",
    "\n",
    "This jupyter notebook, show you how to save and resume a PyTorch Model. In this example we will use the Deep Learning hello-world!: the MNIST classification task.\n",
    "\n",
    "Note: to run code cell you have to press **`Shift + Enter`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "First we need a single point with all the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-71cce6c140a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "import shutil\n",
    "import os.path\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters and Variables\n",
    "\n",
    "Even for Hyper-Parameters and Variables is a good practice have a single point, it's improve code readability and experiments interation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a597b1036219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mbest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "batch_size = 128\n",
    "input_size = 784  # 28 * 28\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 12\n",
    "print_every = 100\n",
    "best_accuracy = torch.FloatTensor([0])\n",
    "start_epoch = 0\n",
    "\n",
    "# Path to saved model weights(as hdf5)\n",
    "resume_weights = \"/model/checkpoint.pth.tar\"\n",
    "\n",
    "# CUDA?\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function\n",
    "\n",
    "In this Cell we have the training, evaluating and save checkpoint function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, test_loader, loss_fn):\n",
    "    \"\"\"Perform a full training over dataset\"\"\"\n",
    "    # Model train mode\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        batch_time = time.time()\n",
    "        # image unrolling\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        if cuda:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Load loss on CPU\n",
    "        if cuda:\n",
    "            loss.cpu()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Measure elapsed time\n",
    "        batch_time = time.time() - batch_time\n",
    "\n",
    "        # ### Keep track of metric every batch\n",
    "        # Accuracy Metric\n",
    "        prediction = outputs.data.max(1)[1]   # first column has actual prob.\n",
    "        accuracy = prediction.eq(labels.data).sum() / batch_size * 100\n",
    "\n",
    "        # Log\n",
    "        if (i + 1) % print_every == 0:\n",
    "            print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f, Accuracy: %.4f, Batch time: %f'\n",
    "                % (epoch + 1,\n",
    "                    num_epochs,\n",
    "                    i + 1,\n",
    "                    len(train_dataset) // batch_size,\n",
    "                    loss.data[0],\n",
    "                    accuracy,\n",
    "                    batch_time))\n",
    "\n",
    "\n",
    "def eval(model, optimizer, test_loader):\n",
    "    \"\"\"Eval over test set\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        # Load output on CPU\n",
    "        if cuda:\n",
    "            output.cpu()\n",
    "        prediction = output.data.max(1)[1]\n",
    "        correct += prediction.eq(target.data).sum()\n",
    "    return correct\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='/output/checkpoint.pth.tar'):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best:\n",
    "        print (\"=> Saving a new best\")\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation Accuracy did not improve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing and Transformation\n",
    "\n",
    "Next, we process the dataset sample in tensor, ready to be feed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels)\n",
    "# If you have not mounted the dataset, you can download it\n",
    "# just adding download=True as parameter\n",
    "train_dataset = dsets.MNIST(root='/input',\n",
    "                        train=True,\n",
    "                        download=True,\n",
    "                        transform=transforms.ToTensor())\n",
    "x_train_mnist, y_train_mnist = train_dataset.train_data.type(torch.FloatTensor), \\\n",
    "                        train_dataset.train_labels\n",
    "test_dataset = dsets.MNIST(root='/input',\n",
    "                        train=False,\n",
    "                        download=True,\n",
    "                        transform=transforms.ToTensor())\n",
    "x_test_mnist, y_test_mnist = test_dataset.test_data.type(torch.FloatTensor), \\\n",
    "                        test_dataset.test_labels\n",
    "\n",
    "# Dataset info\n",
    "print('Training Data Size: ', x_train_mnist.size(), '-', y_train_mnist.size())\n",
    "print('Testing Data Size: ', x_test_mnist.size(), '-', y_test_mnist.size())\n",
    "\n",
    "# Training Dataset Loader (Input Pipline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)\n",
    "# Testing Dataset Loader (Input Pipline)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model\n",
    "\n",
    "A ConvNet Model, state of the art for image classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Model ####\n",
    "# Convolutional Neural Network Model\n",
    "class CNN(nn.Module):\n",
    "\"\"\"Conv[ReLU] -> Conv[ReLU] -> MaxPool -> Dropout(0.25)-\n",
    "-> Flatten -> FC()[ReLU] -> Dropout(0.5) -> FC()[Softmax]\n",
    "\"\"\"\n",
    "def __init__(self, num_classes):\n",
    "    super(CNN, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "    self.drop1 = nn.Dropout2d(p=0.25)\n",
    "    self.fc1 = nn.Linear(9216, 128)\n",
    "    self.drop2 = nn.Dropout2d(p=0.5)\n",
    "    self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "def forward(self, x):\n",
    "    x = F.relu(self.conv1(x))\n",
    "    x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "    x = self.drop1(x)\n",
    "    x = x.view(-1, 9216)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.drop2(x)\n",
    "    x = self.fc2(x)\n",
    "    return F.log_softmax(x)\n",
    "\n",
    "model = CNN(num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume a checkpoint\n",
    "\n",
    "Run the following line if you want to resume an existing checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If exists a best model, load its weights!\n",
    "if os.path.isfile(resume_weights):\n",
    "    print (\"Resumed model's weights from {}\".format(resume_weights))\n",
    "    # load weights\n",
    "    model.load_weights(resume_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The Loss Function and The Optimizers\n",
    "\n",
    "In this example we use the Cross Entropy Loss and Adam Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running a GPU instance, load the model on GPU\n",
    "if cuda:\n",
    "model.cuda()\n",
    "\n",
    "# #### Loss and Optimizer ####\n",
    "# Softmax is internally computed.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# If you are running a GPU instance, compute the loss on GPU\n",
    "if cuda:\n",
    "loss_fn.cuda()\n",
    "\n",
    "# Set parameters to be updated.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Strategy\n",
    "\n",
    "The strategy we have adopted for the this example is the following:\n",
    "- Keep only one checkpoints\n",
    "- Trigger the strategy at the end of every epoch\n",
    "- Save the one with the best(max) validation accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only a single checkpoint, the best over test accuracy.\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                            monitor='val_acc',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Let's train the model and see our checkpoint strategy in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=[checkpoint])\n",
    "\n",
    "# Eval\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume the checkpoint after the training\n",
    "\n",
    "Let's take a look at the checkpoint just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook run in the `/output` folder, so it's here.\n",
    "If you want to load it, go to the Hyper parameters and Varables Code Cell, replace the resume weigths var in this way:\n",
    "`# Path to saved model weights(as hdf5)\n",
    "resume_weights = \"./mnist-cnn-best.hdf5\"`, run the cell, go to the **Resume a checkpoint** Code Cell, run it, and rerun the **Training Code Cell**, that's it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
