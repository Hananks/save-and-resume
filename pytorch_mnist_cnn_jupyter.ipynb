{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Resume a Keras MNIST ConvNet Model\n",
    "\n",
    "This jupyter notebook, show you how to save and resume a PyTorch Model. In this example we will use the Deep Learning hello-world!: the MNIST classification task.\n",
    "\n",
    "Note: to run code cell you have to press **`Shift + Enter`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "First we need a single point with all the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "import shutil\n",
    "import os.path\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters and Variables\n",
    "\n",
    "Even for Hyper-Parameters and Variables is a good practice have a single point, it's improve code readability and experiments interation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "batch_size = 128\n",
    "input_size = 784  # 28 * 28\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 12\n",
    "print_every = 100\n",
    "best_accuracy = torch.FloatTensor([0])\n",
    "start_epoch = 0\n",
    "\n",
    "# Path to saved model weights(as hdf5)\n",
    "resume_weights = \"/model/checkpoint.pth.tar\"\n",
    "\n",
    "resume_weights = \"./checkpoint.pth.tar\"\n",
    "\n",
    "# CUDA?\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function\n",
    "\n",
    "In this Cell we have the training, evaluating and save checkpoint function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, test_loader, loss_fn):\n",
    "    \"\"\"Perform a full training over dataset\"\"\"\n",
    "    average_time = 0\n",
    "    # Model train mode\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        batch_time = time.time()\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        if cuda:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Load loss on CPU\n",
    "        if cuda:\n",
    "            loss.cpu()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Measure elapsed time\n",
    "        batch_time = time.time() - batch_time\n",
    "        # Accumulate over batch\n",
    "        average_time += batch_time\n",
    "\n",
    "        # ### Keep track of metric every batch\n",
    "        # Accuracy Metric\n",
    "        prediction = outputs.data.max(1)[1]   # first column has actual prob.\n",
    "        accuracy = prediction.eq(labels.data).sum() / batch_size * 100\n",
    "\n",
    "        # Log\n",
    "        if (i + 1) % print_every == 0:\n",
    "            print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f, Accuracy: %.4f, Batch time: %f'\n",
    "                % (epoch + 1,\n",
    "                    num_epochs,\n",
    "                    i + 1,\n",
    "                    len(train_dataset) // batch_size,\n",
    "                    loss.data[0],\n",
    "                    accuracy,\n",
    "                    average_time/print_every))  # Average\n",
    "\n",
    "\n",
    "def eval(model, optimizer, test_loader):\n",
    "    \"\"\"Eval over test set\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    # Get Batch\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # Evaluate\n",
    "        output = model(data)\n",
    "        # Load output on CPU\n",
    "        if cuda:\n",
    "            output.cpu()\n",
    "        # Compute Accuracy\n",
    "        prediction = output.data.max(1)[1]\n",
    "        correct += prediction.eq(target.data).sum()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing and Transformation\n",
    "\n",
    "Next, we process the dataset sample in tensor, ready to be feed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "Training Data Size:  torch.Size([60000, 28, 28]) - torch.Size([60000])\n",
      "Testing Data Size:  torch.Size([10000, 28, 28]) - torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset (Images and Labels)\n",
    "# If you have not mounted the dataset, you can download it\n",
    "# just adding download=True as parameter\n",
    "train_dataset = dsets.MNIST(root='/input',\n",
    "                        train=True,\n",
    "                        download=True,\n",
    "                        transform=transforms.ToTensor())\n",
    "x_train_mnist, y_train_mnist = train_dataset.train_data.type(torch.FloatTensor), \\\n",
    "                        train_dataset.train_labels\n",
    "test_dataset = dsets.MNIST(root='/input',\n",
    "                        train=False,\n",
    "                        download=True,\n",
    "                        transform=transforms.ToTensor())\n",
    "x_test_mnist, y_test_mnist = test_dataset.test_data.type(torch.FloatTensor), \\\n",
    "                        test_dataset.test_labels\n",
    "\n",
    "# Dataset info\n",
    "print('Training Data Size: ', x_train_mnist.size(), '-', y_train_mnist.size())\n",
    "print('Testing Data Size: ', x_test_mnist.size(), '-', y_test_mnist.size())\n",
    "\n",
    "# Training Dataset Loader (Input Pipline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)\n",
    "# Testing Dataset Loader (Input Pipline)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model\n",
    "\n",
    "A ConvNet Model, state of the art for image classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN (\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (drop1): Dropout2d (p=0.25)\n",
      "  (fc1): Linear (9216 -> 128)\n",
      "  (drop2): Dropout2d (p=0.5)\n",
      "  (fc2): Linear (128 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# #### Model ####\n",
    "# Convolutional Neural Network Model\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"Conv[ReLU] -> Conv[ReLU] -> MaxPool -> Dropout(0.25)-\n",
    "    -> Flatten -> FC()[ReLU] -> Dropout(0.5) -> FC()[Softmax]\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.drop1 = nn.Dropout2d(p=0.25)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.drop2 = nn.Dropout2d(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = self.drop1(x)\n",
    "        x = x.view(-1, 9216)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = CNN(num_classes)\n",
    "print(model)\n",
    "\n",
    "# If you are running a GPU instance, load the model on GPU\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume a checkpoint\n",
    "\n",
    "Run the following line if you want to resume an existing checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint './checkpoint.pth.tar' ...\n",
      "=> loaded checkpoint './checkpoint.pth.tar' (trained for 10 epochs)\n"
     ]
    }
   ],
   "source": [
    "# If exists a best model, load its weights!\n",
    "if os.path.isfile(resume_weights):\n",
    "    print(\"=> loading checkpoint '{}' ...\".format(resume_weights))\n",
    "    if cuda:\n",
    "        checkpoint = torch.load(resume_weights)\n",
    "    else:\n",
    "        # Load GPU model on CPU\n",
    "        checkpoint = torch.load(resume_weights,\n",
    "                                map_location=lambda storage,\n",
    "                                loc: storage)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_accuracy = checkpoint['best_accuracy']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(resume_weights,\n",
    "        checkpoint['epoch']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The Loss Function and The Optimizers\n",
    "\n",
    "In this example we use the Cross Entropy Loss and Adam Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Loss and Optimizer ####\n",
    "# Softmax is internally computed.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# If you are running a GPU instance, compute the loss on GPU\n",
    "if cuda:\n",
    "    loss_fn.cuda()\n",
    "\n",
    "# Set parameters to be updated.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Strategy\n",
    "\n",
    "The strategy we have adopted for the this example is the following:\n",
    "- Keep only one checkpoints\n",
    "- Trigger the strategy at the end of every epoch\n",
    "- Save the one with the best(max) validation accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only a single checkpoint, the best over test accuracy.\n",
    "def save_checkpoint(state, is_best, filename='/output/checkpoint.pth.tar'):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best:\n",
    "        print (\"=> Saving a new best\")\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation Accuracy did not improve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Let's train the model and see our checkpoint strategy in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/12], Step: [100/468], Loss: 0.0304, Accuracy: 98.4375, Batch time: 0.005967\n",
      "Epoch: [1/12], Step: [200/468], Loss: 0.0331, Accuracy: 99.2188, Batch time: 0.011944\n",
      "Epoch: [1/12], Step: [300/468], Loss: 0.0181, Accuracy: 99.2188, Batch time: 0.017946\n",
      "Epoch: [1/12], Step: [400/468], Loss: 0.0079, Accuracy: 100.0000, Batch time: 0.023949\n",
      "=> Test set: Accuracy: 99.15%\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch: [2/12], Step: [100/468], Loss: 0.0141, Accuracy: 99.2188, Batch time: 0.005985\n",
      "Epoch: [2/12], Step: [200/468], Loss: 0.0186, Accuracy: 99.2188, Batch time: 0.011918\n",
      "Epoch: [2/12], Step: [300/468], Loss: 0.0136, Accuracy: 100.0000, Batch time: 0.017855\n",
      "Epoch: [2/12], Step: [400/468], Loss: 0.0307, Accuracy: 99.2188, Batch time: 0.023823\n",
      "=> Test set: Accuracy: 99.27%\n",
      "=> Saving a new best\n",
      "Epoch: [3/12], Step: [100/468], Loss: 0.0545, Accuracy: 98.4375, Batch time: 0.005938\n",
      "Epoch: [3/12], Step: [200/468], Loss: 0.0043, Accuracy: 100.0000, Batch time: 0.011886\n",
      "Epoch: [3/12], Step: [300/468], Loss: 0.0408, Accuracy: 96.8750, Batch time: 0.017852\n",
      "Epoch: [3/12], Step: [400/468], Loss: 0.0161, Accuracy: 99.2188, Batch time: 0.023796\n",
      "=> Test set: Accuracy: 99.23%\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch: [4/12], Step: [100/468], Loss: 0.0357, Accuracy: 98.4375, Batch time: 0.005919\n",
      "Epoch: [4/12], Step: [200/468], Loss: 0.0415, Accuracy: 99.2188, Batch time: 0.011863\n",
      "Epoch: [4/12], Step: [300/468], Loss: 0.0079, Accuracy: 100.0000, Batch time: 0.017821\n",
      "Epoch: [4/12], Step: [400/468], Loss: 0.0173, Accuracy: 99.2188, Batch time: 0.023815\n",
      "=> Test set: Accuracy: 99.24%\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch: [5/12], Step: [100/468], Loss: 0.0064, Accuracy: 100.0000, Batch time: 0.005956\n",
      "Epoch: [5/12], Step: [200/468], Loss: 0.0075, Accuracy: 100.0000, Batch time: 0.011898\n",
      "Epoch: [5/12], Step: [300/468], Loss: 0.0220, Accuracy: 99.2188, Batch time: 0.017835\n",
      "Epoch: [5/12], Step: [400/468], Loss: 0.0158, Accuracy: 99.2188, Batch time: 0.023799\n",
      "=> Test set: Accuracy: 99.23%\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch: [6/12], Step: [100/468], Loss: 0.0175, Accuracy: 100.0000, Batch time: 0.006003\n",
      "Epoch: [6/12], Step: [200/468], Loss: 0.0097, Accuracy: 99.2188, Batch time: 0.011995\n",
      "Epoch: [6/12], Step: [300/468], Loss: 0.0392, Accuracy: 99.2188, Batch time: 0.017989\n",
      "Epoch: [6/12], Step: [400/468], Loss: 0.0161, Accuracy: 99.2188, Batch time: 0.023942\n",
      "=> Test set: Accuracy: 99.28%\n",
      "=> Saving a new best\n",
      "Epoch: [7/12], Step: [100/468], Loss: 0.0579, Accuracy: 98.4375, Batch time: 0.005972\n",
      "Epoch: [7/12], Step: [200/468], Loss: 0.0248, Accuracy: 99.2188, Batch time: 0.011897\n",
      "Epoch: [7/12], Step: [300/468], Loss: 0.0006, Accuracy: 100.0000, Batch time: 0.017830\n",
      "Epoch: [7/12], Step: [400/468], Loss: 0.0103, Accuracy: 100.0000, Batch time: 0.023758\n",
      "=> Test set: Accuracy: 99.25%\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch: [8/12], Step: [100/468], Loss: 0.0637, Accuracy: 98.4375, Batch time: 0.005992\n",
      "Epoch: [8/12], Step: [200/468], Loss: 0.0023, Accuracy: 100.0000, Batch time: 0.011934\n",
      "Epoch: [8/12], Step: [300/468], Loss: 0.0076, Accuracy: 100.0000, Batch time: 0.017924\n",
      "Epoch: [8/12], Step: [400/468], Loss: 0.0016, Accuracy: 100.0000, Batch time: 0.023878\n",
      "=> Test set: Accuracy: 99.26%\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch: [9/12], Step: [100/468], Loss: 0.0120, Accuracy: 100.0000, Batch time: 0.005922\n",
      "Epoch: [9/12], Step: [200/468], Loss: 0.0008, Accuracy: 100.0000, Batch time: 0.011840\n",
      "Epoch: [9/12], Step: [300/468], Loss: 0.0016, Accuracy: 100.0000, Batch time: 0.017767\n",
      "Epoch: [9/12], Step: [400/468], Loss: 0.0299, Accuracy: 99.2188, Batch time: 0.023730\n",
      "=> Test set: Accuracy: 99.29%\n",
      "=> Saving a new best\n",
      "Epoch: [10/12], Step: [100/468], Loss: 0.0009, Accuracy: 100.0000, Batch time: 0.006006\n",
      "Epoch: [10/12], Step: [200/468], Loss: 0.0075, Accuracy: 100.0000, Batch time: 0.012032\n",
      "Epoch: [10/12], Step: [300/468], Loss: 0.0016, Accuracy: 100.0000, Batch time: 0.018024\n",
      "Epoch: [10/12], Step: [400/468], Loss: 0.0007, Accuracy: 100.0000, Batch time: 0.023979\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, optimizer, train_loader, test_loader, loss_fn)\n",
    "    acc = eval(model, optimizer, test_loader)\n",
    "    acc = 100. * acc / len(test_loader.dataset)\n",
    "    print('=> Test set: Accuracy: {:.2f}%'.format(acc))\n",
    "    acc = torch.FloatTensor([acc])\n",
    "    # Get bool not ByteTensor\n",
    "    is_best = bool(acc.numpy() > best_accuracy.numpy())\n",
    "    # Get greater Tensor to keep track best acc\n",
    "    best_accuracy = torch.FloatTensor(max(acc.numpy(), best_accuracy.numpy()))\n",
    "    # Save checkpoint if is a new best\n",
    "    save_checkpoint({\n",
    "        'epoch': start_epoch + epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_accuracy': best_accuracy\n",
    "    }, is_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume the checkpoint after the training\n",
    "\n",
    "Let's take a look at the checkpoint just created. (you should see the `checkpoint.pth.tar` file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md           keras_mnist_cnn_jupyter.ipynb\r\n",
      "checkpoint.pth.tar  pytorch_mnist_cnn.py\r\n",
      "command.sh          pytorch_mnist_cnn_jupyter.ipynb\r\n",
      "keras_mnist_cnn.py\r\n"
     ]
    }
   ],
   "source": [
    "% ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook run in the `/output` folder, so it's here.\n",
    "If you want to load it, go to the Hyper parameters and Varables Code Cell, replace the resume weigths var in this way:\n",
    "`# Path to saved model weights(as hdf5)\n",
    "resume_weights = \"./checkpoint.pth.tar\"`, run the cell, go to the **Resume a checkpoint** Code Cell, run it, and rerun the **Training Code Cell**, that's it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
