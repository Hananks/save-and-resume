{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Resume a Keras MNIST ConvNet Model\n",
    "\n",
    "This jupyter notebook, show you how to save and resume a Keras Model. In this example we will use the Deep Learning hello-world!: the MNIST classification task.\n",
    "\n",
    "Note: to run code cell you have to press **`Shift + Enter`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "First we need a single point with all the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import os.path\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters and Variables\n",
    "\n",
    "Even for Hyper-Parameters and Variables is a good practice have a single point, this improve code readability and experiments interation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to saved model weights(as hdf5)\n",
    "resume_weights = \"/model/mnist-cnn-best.hdf5\"\n",
    "\n",
    "# Where to save Checkpoint(In the /output folder)\n",
    "filepath = \"/output/mnist-cnn-best.hdf5\"\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing and Transformation\n",
    "\n",
    "Next, we process the dataset sample in tensor, ready to be feed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "10682368/11490434 [==========================>...] - ETA: 0sx_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# MNIST handwritten image classification\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape strategy according to backend\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    # 1 x 28 x 28 [number_of_channels (colors) x height x weight]\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    # 28 x 28 x 1 [height x weight x number_of_channels (colors)]\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Reshape, type, normalized, print\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Dataset info\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model\n",
    "\n",
    "A ConvNet Model, state of the art for image classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# MODEL\n",
    "# Conv(32,3,3)[ReLU] -> Conv(64,3,3)[ReLU] -> MaxPool(2,2)[Dropout 0.25] ->\n",
    "# FC(_, 128)[ReLU][Dropout 0.5] -> FC(128, 10)[Softmax]\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                    activation='relu',\n",
    "                    input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume a checkpoint\n",
    "\n",
    "Run the following line if you want to resume an existing checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed model's weights from ./mnist-cnn-best.hdf5\n"
     ]
    }
   ],
   "source": [
    "# If exists a best model, load its weights!\n",
    "if os.path.isfile(resume_weights):\n",
    "    print (\"Resumed model's weights from {}\".format(resume_weights))\n",
    "    # load weights\n",
    "    model.load_weights(resume_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The Loss Function and The Optimizers\n",
    "\n",
    "In this example we use the Cross Entropy Loss and Adam Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CEE, Adam\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "            optimizer=keras.optimizers.Adam(),\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Strategy\n",
    "\n",
    "The strategy we have adopted for the this example is the following:\n",
    "- Keep only one checkpoints\n",
    "- Trigger the strategy at the end of every epoch\n",
    "- Save the one with the best(max) validation accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only a single checkpoint, the best over test accuracy.\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                            monitor='val_acc',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Let's train the model and see our checkpoint strategy in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9928Epoch 00000: val_acc did not improve\n",
      "60000/60000 [==============================] - 9s - loss: 0.0210 - acc: 0.9929 - val_loss: 0.0309 - val_acc: 0.9912\n",
      "Epoch 2/12\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9931Epoch 00001: val_acc did not improve\n",
      "60000/60000 [==============================] - 9s - loss: 0.0207 - acc: 0.9931 - val_loss: 0.0248 - val_acc: 0.9927\n",
      "Epoch 3/12\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9934Epoch 00002: val_acc did not improve\n",
      "60000/60000 [==============================] - 9s - loss: 0.0205 - acc: 0.9934 - val_loss: 0.0270 - val_acc: 0.9922\n",
      "Epoch 4/12\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9939Epoch 00003: val_acc did not improve\n",
      "60000/60000 [==============================] - 9s - loss: 0.0186 - acc: 0.9940 - val_loss: 0.0279 - val_acc: 0.9928\n",
      "Epoch 5/12\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9944Epoch 00004: val_acc did not improve\n",
      "60000/60000 [==============================] - 9s - loss: 0.0171 - acc: 0.9944 - val_loss: 0.0273 - val_acc: 0.9924\n",
      "Epoch 6/12\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9948Epoch 00005: val_acc did not improve\n",
      "60000/60000 [==============================] - 9s - loss: 0.0153 - acc: 0.9948 - val_loss: 0.0289 - val_acc: 0.9928\n",
      "Epoch 7/12\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9944Epoch 00006: val_acc improved from 0.99300 to 0.99360, saving model to /output/mnist-cnn-best.hdf5\n",
      "60000/60000 [==============================] - 9s - loss: 0.0157 - acc: 0.9944 - val_loss: 0.0296 - val_acc: 0.9936\n",
      "Epoch 8/12\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9949Epoch 00007: val_acc did not improve\n",
      "60000/60000 [==============================] - 9s - loss: 0.0145 - acc: 0.9949 - val_loss: 0.0281 - val_acc: 0.9928\n",
      "Epoch 9/12\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9953Epoch 00008: val_acc did not improve\n",
      "60000/60000 [==============================] - 9s - loss: 0.0152 - acc: 0.9953 - val_loss: 0.0278 - val_acc: 0.9927\n",
      "Epoch 10/12\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9958Epoch 00009: val_acc did not improve\n",
      "60000/60000 [==============================] - 9s - loss: 0.0127 - acc: 0.9959 - val_loss: 0.0303 - val_acc: 0.9926\n",
      "Epoch 11/12\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9955Epoch 00010: val_acc did not improve\n",
      "60000/60000 [==============================] - 9s - loss: 0.0135 - acc: 0.9956 - val_loss: 0.0296 - val_acc: 0.9931\n",
      "Epoch 12/12\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9955Epoch 00011: val_acc improved from 0.99360 to 0.99380, saving model to /output/mnist-cnn-best.hdf5\n",
      "60000/60000 [==============================] - 9s - loss: 0.0130 - acc: 0.9954 - val_loss: 0.0276 - val_acc: 0.9938\n",
      "Test loss: 0.0276465165614\n",
      "Test accuracy: 0.9938\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=[checkpoint])\n",
    "\n",
    "# Eval\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume the checkpoint after the training\n",
    "\n",
    "Let's take a look at the checkpoint just created. (you should see the `mnist-cnn-best.hdf5` file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mMNIST_data\u001b[0m/     command.sh                     mnist-cnn-best.hdf5\r\n",
      "README.md       keras_mnist_cnn.py             pytorch_mnist_cnn.py\r\n",
      "Untitled.ipynb  keras_mnist_cnn_jupyter.ipynb  pytorch_mnist_cnn_jupyter.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "% ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook run in the `/output` folder, so it's here.\n",
    "If you want to load it, go to the Hyper parameters and Varables Code Cell, replace the resume weigths var in this way:\n",
    "`# Path to saved model weights(as hdf5)\n",
    "resume_weights = \"./mnist-cnn-best.hdf5\"`, run the cell, go to the **Resume a checkpoint** Code Cell, run it, and rerun the **Training Code Cell**, that's it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
